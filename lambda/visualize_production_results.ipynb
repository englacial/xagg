{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Visualize Lambda Production Results\n",
    "\n",
    "This notebook reads parquet files from the Lambda production run and visualizes them using xdggs/xarray.\n",
    "\n",
    "**Input:** Parquet files in `s3://{bucket}/{prefix}/{morton}.parquet`\n",
    "\n",
    "**Columns in each parquet file:**\n",
    "- `child_morton`: Morton index at order 12\n",
    "- `child_healpix`: HEALPix cell ID at order 12\n",
    "- `count`: Number of observations\n",
    "- `h_mean`: Weighted mean elevation\n",
    "- `h_sigma`: Uncertainty in mean\n",
    "- `h_min`, `h_max`: Elevation range\n",
    "- `h_variance`, `h_q25`, `h_q50`, `h_q75`: Distribution stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import xdggs\n",
    "import s3fs\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nS3_BUCKET = \"xagg\"\nS3_PREFIX = \"atl06/production\"\nCHILD_ORDER = 12\n\nprint(f\"Reading from: s3://{S3_BUCKET}/{S3_PREFIX}/\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. List and Load Parquet Files from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to S3\n",
    "s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "# List all parquet files\n",
    "parquet_files = s3.glob(f\"{S3_BUCKET}/{S3_PREFIX}/*.parquet\")\n",
    "print(f\"Found {len(parquet_files)} parquet files\")\n",
    "\n",
    "if parquet_files:\n",
    "    print(f\"\\nFirst 5 files:\")\n",
    "    for f in sorted(parquet_files)[:5]:\n",
    "        print(f\"  s3://{f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all parquet files into a single DataFrame\n",
    "print(f\"Reading {len(parquet_files)} parquet files...\")\n",
    "\n",
    "all_dfs = []\n",
    "for i, parquet_path in enumerate(sorted(parquet_files)):\n",
    "    try:\n",
    "        with s3.open(parquet_path, 'rb') as f:\n",
    "            df = pd.read_parquet(f)\n",
    "            # Only keep rows with data\n",
    "            df = df[df['count'] > 0]\n",
    "            if len(df) > 0:\n",
    "                all_dfs.append(df)\n",
    "        \n",
    "        if (i + 1) % 200 == 0:\n",
    "            print(f\"  Loaded {i + 1}/{len(parquet_files)} files...\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading {parquet_path}: {e}\")\n",
    "\n",
    "# Combine all DataFrames\n",
    "df_all = pd.concat(all_dfs, ignore_index=True)\n",
    "print(f\"\\nLoaded {len(df_all):,} cells with data from {len(all_dfs)} files\")\n",
    "print(f\"\\nDataFrame shape: {df_all.shape}\")\n",
    "print(f\"Columns: {list(df_all.columns)}\")\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Convert to xdggs Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create xarray Dataset with HEALPix cell_ids\n",
    "cell_ids = df_all['child_healpix'].values\n",
    "\n",
    "ds = xr.Dataset(\n",
    "    data_vars={\n",
    "        'count': ('cell_ids', df_all['count'].values.astype(np.int32)),\n",
    "        'h_mean': ('cell_ids', df_all['h_mean'].values.astype(np.float32)),\n",
    "        'h_sigma': ('cell_ids', df_all['h_sigma'].values.astype(np.float32)),\n",
    "        'h_min': ('cell_ids', df_all['h_min'].values.astype(np.float32)),\n",
    "        'h_max': ('cell_ids', df_all['h_max'].values.astype(np.float32)),\n",
    "        'h_variance': ('cell_ids', df_all['h_variance'].values.astype(np.float32)),\n",
    "        'h_q25': ('cell_ids', df_all['h_q25'].values.astype(np.float32)),\n",
    "        'h_q50': ('cell_ids', df_all['h_q50'].values.astype(np.float32)),\n",
    "        'h_q75': ('cell_ids', df_all['h_q75'].values.astype(np.float32)),\n",
    "    },\n",
    "    coords={\n",
    "        'cell_ids': (\n",
    "            'cell_ids',\n",
    "            cell_ids,\n",
    "            {'grid_name': 'healpix', 'level': CHILD_ORDER, 'indexing_scheme': 'nested'}\n",
    "        ),\n",
    "        'morton': ('cell_ids', df_all['child_morton'].values)\n",
    "    },\n",
    "    attrs={\n",
    "        'title': 'ATL06 Lambda Production Results',\n",
    "        'child_order': CHILD_ORDER,\n",
    "        'grid_type': 'healpix',\n",
    "        'indexing_scheme': 'nested',\n",
    "        'source': f's3://{S3_BUCKET}/{S3_PREFIX}/'\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Created xarray Dataset:\")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode with xdggs to enable DGGS operations\n",
    "ds = xdggs.decode(ds, index_options={\"index_kind\": \"moc\"})\n",
    "\n",
    "# Add lat/lon coordinates\n",
    "ds = ds.dggs.assign_latlon_coords()\n",
    "\n",
    "print(\"Decoded with xdggs and added lat/lon coords:\")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nTotal cells with data: {len(ds['cell_ids']):,}\")\n",
    "print(f\"Total observations: {ds['count'].sum().values:,}\")\n",
    "\n",
    "print(f\"\\nElevation (h_mean):\")\n",
    "print(f\"  Min:  {ds['h_mean'].min().values:.2f} m\")\n",
    "print(f\"  Max:  {ds['h_mean'].max().values:.2f} m\")\n",
    "print(f\"  Mean: {ds['h_mean'].mean().values:.2f} m\")\n",
    "print(f\"  Std:  {ds['h_mean'].std().values:.2f} m\")\n",
    "\n",
    "print(f\"\\nUncertainty (h_sigma):\")\n",
    "print(f\"  Min:  {ds['h_sigma'].min().values:.4f} m\")\n",
    "print(f\"  Max:  {ds['h_sigma'].max().values:.2f} m\")\n",
    "print(f\"  Mean: {ds['h_sigma'].mean().values:.4f} m\")\n",
    "\n",
    "print(f\"\\nObservation counts per cell:\")\n",
    "print(f\"  Min:  {ds['count'].min().values}\")\n",
    "print(f\"  Max:  {ds['count'].max().values:,}\")\n",
    "print(f\"  Mean: {ds['count'].mean().values:.1f}\")\n",
    "\n",
    "print(f\"\\nCoverage:\")\n",
    "print(f\"  Lat: {ds['latitude'].min().values:.2f} to {ds['latitude'].max().values:.2f}\")\n",
    "print(f\"  Lon: {ds['longitude'].min().values:.2f} to {ds['longitude'].max().values:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. Visualization - Antarctic Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antarctic Polar Stereographic projection\n",
    "proj = ccrs.SouthPolarStereo()\n",
    "data_crs = ccrs.PlateCarree()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 16), \n",
    "                          subplot_kw={'projection': proj})\n",
    "\n",
    "# Add Antarctic coastline to all subplots\n",
    "for ax in axes.flat:\n",
    "    ax.coastlines(resolution='50m', linewidth=0.5)\n",
    "    ax.add_feature(cfeature.LAND, facecolor='lightgray', alpha=0.3)\n",
    "    ax.gridlines(draw_labels=False, alpha=0.3)\n",
    "    ax.set_extent([-180, 180, -90, -60], crs=data_crs)\n",
    "\n",
    "# 1. Mean elevation\n",
    "ax = axes[0, 0]\n",
    "valid = ~np.isnan(ds['h_mean'].values)\n",
    "scatter = ax.scatter(\n",
    "    ds['longitude'].values[valid],\n",
    "    ds['latitude'].values[valid],\n",
    "    c=ds['h_mean'].values[valid],\n",
    "    s=0.5, cmap='terrain', alpha=0.8,\n",
    "    vmin=0, vmax=4000,\n",
    "    transform=data_crs\n",
    ")\n",
    "ax.set_title(f'Mean Elevation ({np.sum(valid):,} cells)', fontsize=14, weight='bold')\n",
    "plt.colorbar(scatter, ax=ax, label='Elevation (m)', shrink=0.7)\n",
    "\n",
    "# 2. Observation count\n",
    "ax = axes[0, 1]\n",
    "valid = ds['count'].values > 0\n",
    "scatter = ax.scatter(\n",
    "    ds['longitude'].values[valid],\n",
    "    ds['latitude'].values[valid],\n",
    "    c=ds['count'].values[valid],\n",
    "    s=0.5, cmap='viridis', alpha=0.8,\n",
    "    norm=plt.matplotlib.colors.LogNorm(vmin=1),\n",
    "    transform=data_crs\n",
    ")\n",
    "ax.set_title(f'Observation Count ({np.sum(valid):,} cells)', fontsize=14, weight='bold')\n",
    "plt.colorbar(scatter, ax=ax, label='Count (log scale)', shrink=0.7)\n",
    "\n",
    "# 3. Uncertainty (sigma)\n",
    "ax = axes[1, 0]\n",
    "valid = ~np.isnan(ds['h_sigma'].values)\n",
    "scatter = ax.scatter(\n",
    "    ds['longitude'].values[valid],\n",
    "    ds['latitude'].values[valid],\n",
    "    c=ds['h_sigma'].values[valid],\n",
    "    s=0.5, cmap='plasma', alpha=0.8,\n",
    "    vmax=1.0,\n",
    "    transform=data_crs\n",
    ")\n",
    "ax.set_title(f'Uncertainty (h_sigma) ({np.sum(valid):,} cells)', fontsize=14, weight='bold')\n",
    "plt.colorbar(scatter, ax=ax, label='Uncertainty (m)', shrink=0.7)\n",
    "\n",
    "# 4. Elevation range (max - min)\n",
    "ax = axes[1, 1]\n",
    "h_range = ds['h_max'].values - ds['h_min'].values\n",
    "valid = ~np.isnan(h_range)\n",
    "scatter = ax.scatter(\n",
    "    ds['longitude'].values[valid],\n",
    "    ds['latitude'].values[valid],\n",
    "    c=h_range[valid],\n",
    "    s=0.5, cmap='hot', alpha=0.8,\n",
    "    vmax=100,\n",
    "    transform=data_crs\n",
    ")\n",
    "ax.set_title(f'Elevation Range (max-min) ({np.sum(valid):,} cells)', fontsize=14, weight='bold')\n",
    "plt.colorbar(scatter, ax=ax, label='Range (m)', shrink=0.7)\n",
    "\n",
    "plt.suptitle('ATL06 Lambda Production Results - Cycle 22', fontsize=16, weight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6. Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Elevation distribution\n",
    "ax = axes[0, 0]\n",
    "valid = ~np.isnan(ds['h_mean'].values)\n",
    "ax.hist(ds['h_mean'].values[valid], bins=100, edgecolor='none', alpha=0.7)\n",
    "ax.set_xlabel('Elevation (m)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Mean Elevation Distribution')\n",
    "ax.axvline(ds['h_mean'].mean().values, color='red', linestyle='--', label=f'Mean: {ds[\"h_mean\"].mean().values:.0f}m')\n",
    "ax.legend()\n",
    "\n",
    "# Observation count distribution (log scale)\n",
    "ax = axes[0, 1]\n",
    "valid = ds['count'].values > 0\n",
    "ax.hist(np.log10(ds['count'].values[valid]), bins=50, edgecolor='none', alpha=0.7)\n",
    "ax.set_xlabel('log10(Observation Count)')\n",
    "ax.set_ylabel('Number of Cells')\n",
    "ax.set_title('Observation Count Distribution')\n",
    "\n",
    "# Uncertainty distribution\n",
    "ax = axes[1, 0]\n",
    "valid = ~np.isnan(ds['h_sigma'].values) & (ds['h_sigma'].values < 10)\n",
    "ax.hist(ds['h_sigma'].values[valid], bins=100, edgecolor='none', alpha=0.7)\n",
    "ax.set_xlabel('Uncertainty (m)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Uncertainty Distribution (h_sigma < 10m)')\n",
    "\n",
    "# Latitude distribution\n",
    "ax = axes[1, 1]\n",
    "ax.hist(ds['latitude'].values, bins=50, edgecolor='none', alpha=0.7)\n",
    "ax.set_xlabel('Latitude')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Latitude Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 7. Regional Zoom - West Antarctica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom into West Antarctica (Thwaites/Pine Island region)\n",
    "proj = ccrs.SouthPolarStereo()\n",
    "data_crs = ccrs.PlateCarree()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10), subplot_kw={'projection': proj})\n",
    "\n",
    "ax.coastlines(resolution='10m', linewidth=0.5)\n",
    "ax.add_feature(cfeature.LAND, facecolor='lightgray', alpha=0.3)\n",
    "ax.gridlines(draw_labels=True, alpha=0.3)\n",
    "\n",
    "# West Antarctica extent\n",
    "ax.set_extent([-140, -70, -85, -70], crs=data_crs)\n",
    "\n",
    "valid = ~np.isnan(ds['h_mean'].values)\n",
    "scatter = ax.scatter(\n",
    "    ds['longitude'].values[valid],\n",
    "    ds['latitude'].values[valid],\n",
    "    c=ds['h_mean'].values[valid],\n",
    "    s=2, cmap='terrain', alpha=0.9,\n",
    "    vmin=0, vmax=2500,\n",
    "    transform=data_crs\n",
    ")\n",
    "\n",
    "plt.colorbar(scatter, ax=ax, label='Elevation (m)', shrink=0.7)\n",
    "ax.set_title('West Antarctica - Mean Elevation (Thwaites/Pine Island Region)', \n",
    "             fontsize=14, weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 8. Save Combined Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally save combined dataset to zarr\n",
    "# output_path = f\"s3://{S3_BUCKET}/{S3_PREFIX}/combined.zarr\"\n",
    "# ds.to_zarr(output_path, mode='w')\n",
    "# print(f\"Saved to: {output_path}\")\n",
    "\n",
    "# Or save locally\n",
    "# ds.to_zarr(\"production_results_combined.zarr\", mode='w')\n",
    "# print(\"Saved to: production_results_combined.zarr\")\n",
    "\n",
    "print(\"To save the combined dataset, uncomment the code above.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}